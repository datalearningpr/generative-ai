{"cells":[{"cell_type":"markdown","metadata":{"id":"zX436BXvo9_i"},"source":["# Create Voice Over for Video using LLaVa and TTS\n","\n","This notebook utilizes open source vision and speech synthesis to automatically create voiceover narration for a video file. The key steps are:\n","\n","1. **Extract Video Frames**: Utilize OpenCV, an open source computer vision library, to sample frames from the input video file at regular intervals such as every 25 frames.\n","\n","2. **Generate Image Descriptions**: Feed the extracted video frames into LLaVA, an open source vision model, to generate textual descriptions of the visual contents of each frame.\n","\n","3. **Summarize Frame Descriptions**: Take the LLaVA-generated descriptions for all sampled frames and process them into a prompt call to GPT-4 for voiceover narration covering the full video contents.\n","\n","4. **Synthesize Narration Audio**: Use SileroTTS, an open source text-to-speech model, to synthesize the narration audio from the generated script.\n","\n","5. **Combine Audio and Video**: Finally, mix the synthesized narration audio track with the original video to produce the final video with automatic voiceover.\n","\n","The use of open source vision and speech models provides an efficient way to automatically create narration for video contents without manual effort."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":38628,"status":"ok","timestamp":1699794545277,"user":{"displayName":"cloud aws","userId":"00203203857429609385"},"user_tz":-480},"id":"GPZeOjVPpRsC","outputId":"8437f935-686c-4b6c-8c0c-8f257faf0a57"},"outputs":[],"source":["!pip install openai --quiet\n","!pip install langchain --quiet\n","!pip install pydub loguru --quiet\n","!pip install moviepy --quiet\n","!pip install opencv-python --quiet\n","!pip install replicate --quiet"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"glxngAAxpaGh"},"outputs":[],"source":["!mkdir frames"]},{"cell_type":"markdown","metadata":{"id":"0wMO_XiRrEDT"},"source":["### text to speech service, opne source silero is used here, code sample referred to https://github.com/ouoertheo/silero-api-server"]},{"cell_type":"code","execution_count":27,"metadata":{"executionInfo":{"elapsed":8310,"status":"ok","timestamp":1699794562534,"user":{"displayName":"cloud aws","userId":"00203203857429609385"},"user_tz":-480},"id":"PmDQ4aCNkDqJ"},"outputs":[],"source":["import os, time\n","import shutil\n","import requests\n","import torch\n","import torch.package\n","import torchaudio\n","from hashlib import md5\n","from loguru import logger\n","from pydub import AudioSegment\n","from pathlib import Path\n","import json\n","\n","class SileroTtsService:\n","    \"\"\"\n","    Generate TTS wav files using Silero\n","    \"\"\"\n","    def __init__(self, sample_path, lang=\"v3_en.pt\") -> None:\n","        self.sample_text = \"The fallowed fallen swindle auspacious goats in portable power stations.\"\n","        self.sample_path = Path(sample_path)\n","        self.sessions_path = None\n","\n","        # Silero works fine on CPU\n","        self.device = torch.device('cpu')\n","        torch.set_num_threads(4)\n","        torchaudio.set_audio_backend(\"soundfile\")\n","\n","        # Make sure we have the sample path\n","        if not self.sample_path.exists():\n","            self.sample_path.mkdir()\n","\n","        self.sample_rate = 48000\n","        logger.info(f\"TTS Service loaded successfully\")\n","\n","        # Prevent generation failure due to too long input\n","        self.max_char_length = 600\n","\n","        # Get language model URLs\n","        self.langs = self.list_languages()\n","\n","        # Load model\n","        self.load_model(lang)\n","\n","    def init_sessions_path(self, sessions_path=\"sessions\"):\n","        self.sessions_path = Path(sessions_path)\n","        if not self.sessions_path.exists():\n","            self.sessions_path.mkdir()\n","\n","    def load_model(self, lang_model=\"v3_en.pt\"):\n","        # Download the model. Default to en.\n","        if lang_model not in self.langs:\n","            raise Exception(f\"{lang_model} not in {list(self.langs.values())}\")\n","\n","        model_url = self.langs[lang_model]\n","        self.model_file = Path(lang_model)\n","\n","        if not Path.is_file(self.model_file):\n","            logger.warning(f\"Downloading Silero {lang_model} model...\")\n","            torch.hub.download_url_to_file(model_url,\n","                                        self.model_file)\n","            logger.info(f\"Model download completed.\")\n","\n","        self.model = torch.package.PackageImporter(self.model_file).load_pickle(\"tts_models\", \"model\")\n","        self.model.to(self.device)\n","\n","    def generate(self, speaker, text, session=\"\"):\n","        if len(text) > self.max_char_length:\n","            # Handle long text input\n","            text_chunks = self.split_text(text)\n","            combined_wav = AudioSegment.empty()\n","\n","            for chunk in text_chunks:\n","                audio_path = Path(self.model.save_wav(text=chunk,speaker=speaker,sample_rate=self.sample_rate))\n","                combined_wav += AudioSegment.silent(500) # Insert 500ms pause\n","                combined_wav += AudioSegment.from_file(audio_path)\n","\n","            combined_wav.export(\"test.wav\", format=\"wav\")\n","            audio_path = Path(\"test.wav\")\n","        else:\n","            audio_path = Path(self.model.save_wav(text=text,speaker=speaker,sample_rate=self.sample_rate))\n","        if session:\n","            self.save_session_audio(audio_path, session, speaker)\n","        return audio_path\n","\n","    def split_text(self, text:str) -> list[str]:\n","        # Split text into chunks less than self.max_char_length\n","        chunk_list = []\n","        chunk_str = \"\"\n","\n","        for word in text.split(' '):\n","            word = word.replace('\\n',' ') + \" \"\n","            if len(chunk_str + word) > self.max_char_length:\n","                chunk_list.append(chunk_str)\n","                chunk_str = \"\"\n","            chunk_str += word\n","\n","        # Add the last chunk\n","        if len(chunk_str) > 0:\n","            chunk_list.append(chunk_str)\n","\n","        return chunk_list\n","\n","\n","    def combine_audio(self, audio_segments):\n","        combined_audio = AudioSegment.from_mono_audiosegments(audio_segments)\n","        return combined_audio\n","\n","    def save_session_audio(self, audio_path:Path, session:Path, speaker):\n","        if not self.sessions_path:\n","            raise Exception(\"Session not initialized. Call /tts/init_session with {'path':'desired\\session\\path'}\")\n","        session_path = self.sessions_path.joinpath(session)\n","        if not session_path.exists():\n","            session_path.mkdir()\n","        dst = session_path.joinpath(f\"tts_{session}_{int(time.time())}_{speaker}_.wav\")\n","        shutil.copy(audio_path, dst)\n","\n","    def get_speakers(self):\n","        \"List different speakers in model\"\n","        return self.model.speakers\n","\n","    def generate_samples(self):\n","        \"Remove current samples and generate new ones for all speakers.\"\n","        logger.warning(\"Removing current samples\")\n","        for file in self.sample_path.iterdir():\n","            os.remove(self.sample_path.joinpath(file))\n","\n","        logger.info(\"Creating new samples. This should take a minute...\")\n","        for speaker in self.model.speakers:\n","            sample_name = Path(self.sample_path.joinpath(f\"{speaker}.wav\"))\n","            if sample_name.exists():\n","                continue\n","            audio = Path(self.model.save_wav(text=self.sample_text,speaker=speaker,sample_rate=self.sample_rate))\n","            audio.rename(self.sample_path.joinpath(sample_name))\n","        logger.info(\"New samples created\")\n","\n","    def update_sample_text(self,text: str):\n","        \"Update the text used to generate samples\"\n","        if not text: return\n","        self.sample_text = text\n","        logger.info(f\"Sample text updated to {self.sample_text}\")\n","\n","    def list_languages(self):\n","        'Grab all v3 model links from https://models.silero.ai/models/tts'\n","        lang_file = Path('langs.json')\n","        if lang_file.exists():\n","            with lang_file.open('r') as fh:\n","                logger.info('Loading cached language index')\n","                return json.load(fh)\n","        logger.info('Loading remote language index')\n","        lang_base_url = 'https://models.silero.ai/models/tts'\n","        lang_urls = {}\n","\n","        # Parse initial web directory for languages\n","        response = requests.get(lang_base_url)\n","        langs = [lang.split('/')[0] for lang in response.text.split('<a href=\"')][1:]\n","\n","        # Enter each web directory and grab v3 model file links\n","        for lang in langs:\n","            response = requests.get(f\"{lang_base_url}/{lang}\")\n","            if not response.ok:\n","                raise f\"Failed to get languages: {response.status_code}\"\n","            lang_files = [f.split('\"')[0] for f in response.text.split('<a href=\"')][1:]\n","\n","            # If a valid v3 file, add to list\n","            for lang_file in lang_files:\n","                if lang_file.startswith('v3'):\n","                    lang_urls[lang_file]=f\"{lang_base_url}/{lang}/{lang_file}\"\n","        with open('langs.json','w') as fh:\n","            json.dump(lang_urls,fh)\n","        return lang_urls"]},{"cell_type":"markdown","metadata":{"id":"G2OnGjRc8FDi"},"source":["### function to extract frames from video"]},{"cell_type":"markdown","metadata":{"id":"0fjCqA8y8Eyp"},"source":[]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":304,"status":"ok","timestamp":1699793509768,"user":{"displayName":"cloud aws","userId":"00203203857429609385"},"user_tz":-480},"id":"KwF_S3qflosa"},"outputs":[],"source":["import cv2\n","def video_to_frames(video_file, output_dir):\n","    vidcap = cv2.VideoCapture(video_file)\n","    fps = vidcap.get(cv2.CAP_PROP_FPS)\n","    success, image = vidcap.read()\n","    count = 0\n","    while success:\n","        cv2.imwrite(output_dir + f\"/frame{count}.jpg\", image)\n","        success, image = vidcap.read()\n","        count += 1\n","    return count, count/fps\n"]},{"cell_type":"markdown","metadata":{"id":"lSiAKz-g8Ns-"},"source":["### environment variables"]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":34,"status":"ok","timestamp":1699794079762,"user":{"displayName":"cloud aws","userId":"00203203857429609385"},"user_tz":-480},"id":"JfrWFCy4nJsi"},"outputs":[],"source":["import os\n","os.environ[\"REPLICATE_API_TOKEN\"] = \"xxxx\"\n","os.environ[\"OPENAI_API_KEY\"] = \"xxxx\"\n","os.environ[\"OPENAI_API_BASE\"] = \"xxx\"\n","os.environ[\"OPENAI_API_VERSION\"] = \"xxxx\"\n"]},{"cell_type":"markdown","metadata":{"id":"J1-Z9Wgn8TrU"},"source":["### use vision model to get text from image of different sample frames"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":331,"status":"ok","timestamp":1699793969231,"user":{"displayName":"cloud aws","userId":"00203203857429609385"},"user_tz":-480},"id":"RsBt5t0jnBEI"},"outputs":[],"source":["import replicate\n","def frames_to_frame_text(count):\n","\n","    target_ids = list(range(count))[0::25]\n","\n","    frame_texts = []\n","\n","    for i in target_ids:\n","        output = replicate.run(\n","            \"yorickvp/llava-13b:2facb4a474a0462c15041b78b1ad70952ea46b5ec6ad29583c0b29dbd4249591\",\n","            input={\n","                \"image\": open(f\"frames/frame{i}.jpg\", \"rb\"),\n","                # \"image\": \"https://marketplace.canva.com/EAETpJ0lmjg/2/0/1131w/canva-fashion-invoice-in-beige-black-minimalist-style-zvoLwRH8Wys.jpg\",\n","                \"prompt\": f\"this is the frame {i+1} of an video, describe this frame.\"\n","            }\n","        )\n","        ret = \"\".join(list(output))\n","        frame_texts.append(ret)\n","\n","\n","    text = \"\"\n","\n","    for i, id in enumerate(target_ids):\n","        text += f\"frame {id} description:\\n\"\n","        text += frame_texts[i] + \"\\n------\\n\\n\\n\"\n","\n","    return text\n"]},{"cell_type":"markdown","metadata":{"id":"1EEgrOcm8cgs"},"source":["### use a prompt to get voice over text from the series of frame texts"]},{"cell_type":"code","execution_count":18,"metadata":{"executionInfo":{"elapsed":825,"status":"ok","timestamp":1699794197939,"user":{"displayName":"cloud aws","userId":"00203203857429609385"},"user_tz":-480},"id":"_qBwmr9Un3ZG"},"outputs":[],"source":["\n","from langchain.prompts import PromptTemplate\n","from langchain.chat_models import AzureChatOpenAI\n","from langchain.schema import StrOutputParser\n","\n","\n","def frame_text_to_voice_over(text, duration):\n","\n","    prompt = PromptTemplate.from_template(\n","        \"\"\"\n","        we have a video, below it is the description of sampled frames(every 25):\n","\n","        ### description of different sample frames\n","        {text}\n","\n","        ### Instruction\n","        Generate a Voice-over text for this video based on the descriptions of frames above.\n","        The text should not have more than {word_count} words\n","        \"\"\"\n","    )\n","    runnable = prompt | AzureChatOpenAI(deployment_name=\"gpt-4\") | StrOutputParser()\n","    ret = runnable.invoke({\"text\": text,\n","                    #  \"duration\": duration,\n","                    \"word_count\": duration * 2})\n","    return ret\n","\n"]},{"cell_type":"markdown","metadata":{"id":"qDGSHakR8l1Z"},"source":["### combine the audio with the video"]},{"cell_type":"code","execution_count":30,"metadata":{"executionInfo":{"elapsed":292,"status":"ok","timestamp":1699794997603,"user":{"displayName":"cloud aws","userId":"00203203857429609385"},"user_tz":-480},"id":"AE2H0zQIrR-A"},"outputs":[],"source":["\n","def combine_audio(vidname, audname, outname, fps=25):\n","    import moviepy.editor as mpe\n","    my_clip = mpe.VideoFileClip(vidname)\n","    audio_background = mpe.AudioFileClip(audname)\n","    final_clip = my_clip.set_audio(audio_background)\n","    final_clip.write_videofile(outname,fps=fps)\n"]},{"cell_type":"markdown","metadata":{"id":"YgVKBsdf8rzO"},"source":["### with an input video, use the functions step by step to create a voice over for the video"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"68qZ5Ny0Xxwa"},"outputs":[],"source":["\n","tts_service = SileroTtsService(\"./samples\")\n","\n","x = \"input video locaiton\"\n","\n","import glob\n","files = glob.glob('./frames/*')\n","for f in files:\n","    os.remove(f)\n","count, duration = video_to_frames(x, \"frames\")\n","frame_text = frames_to_frame_text(count)\n","voice_over = frame_text_to_voice_over(frame_text, duration)\n","tts_service.generate(speaker = f'en_{49}', text=voice_over)\n","combine_audio(x, \"test.wav\", \"output.mp4\")\n"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
