{"cells":[{"cell_type":"markdown","metadata":{"id":"zX436BXvo9_i"},"source":["# Chat with an AI Assistant via Audio\n","\n","This colab notebook allows you to have a conversation with an AI assistant using audio inputs and outputs. The assistant is powered by Open source LLM `Mistral-7b-instruct`, you can swtich to another model you want.\n","\n","## How it works\n","\n","- User audio input is recorded and converted to text using `Whisper`.\n","- The text is sent to LLM to generate a text response.\n","- LLM text response is converted to speech using `Silero-tts`.\n","- The audio output is played back to the user.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GPZeOjVPpRsC"},"outputs":[],"source":["!pip install gradio==3.48 --quiet\n","!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python --quiet\n","!pip install langchain --quiet\n","!pip install huggingface-hub --quiet"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25473,"status":"ok","timestamp":1698587177511,"user":{"displayName":"cloud aws","userId":"00203203857429609385"},"user_tz":-480},"id":"rUsrXhG5khOg","outputId":"3dbc54c1-1ed2-4ebd-8e32-f99a4c5725f8"},"outputs":[],"source":["!pip install git+https://github.com/openai/whisper.git loguru pydub--quiet\n","!sudo apt update && sudo apt install ffmpeg"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1059,"status":"ok","timestamp":1698587178034,"user":{"displayName":"cloud aws","userId":"00203203857429609385"},"user_tz":-480},"id":"nA-J1tlkkkxV","outputId":"a58b57c7-9d0a-4f51-8fd5-e194cd0054f0"},"outputs":[],"source":["# installing the whisper will mess up the torchaudio version with the torch version, have to install again\n","!pip install torchaudio==2.0.1 --queit"]},{"cell_type":"markdown","metadata":{"id":"njgWGMVcqRiv"},"source":["### download the model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":41018,"status":"ok","timestamp":1698587219048,"user":{"displayName":"cloud aws","userId":"00203203857429609385"},"user_tz":-480},"id":"HVODLGfe780J","outputId":"cecfa57f-8ba3-42a9-c3f4-bed3cdca4428"},"outputs":[],"source":["!huggingface-cli download TheBloke/Mistral-7B-Instruct-v0.1-GGUF mistral-7b-instruct-v0.1.Q4_K_M.gguf --local-dir . --local-dir-use-symlinks False"]},{"cell_type":"markdown","metadata":{"id":"LH-_xlnEqY4q"},"source":["### load the model using the langchain llmacpp wrapper"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Tb-zfY9qqA3X"},"outputs":[],"source":["from langchain.llms import LlamaCpp\n","from langchain.prompts import PromptTemplate\n","from langchain.chains import LLMChain\n","from langchain.callbacks.manager import CallbackManager\n","from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25119,"status":"ok","timestamp":1698589804024,"user":{"displayName":"cloud aws","userId":"00203203857429609385"},"user_tz":-480},"id":"OzPsQPeFjvwb","outputId":"ca0bac48-a195-4574-833c-7c8ebdcd8095"},"outputs":[],"source":["n_gpu_layers = 50\n","n_batch = 512\n","callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n","\n","llm = LlamaCpp(\n","    model_path=\"mistral-7b-instruct-v0.1.Q4_K_M.gguf\",\n","    n_gpu_layers=n_gpu_layers,\n","    temperature=0,\n","    # n_batch=n_batch,\n","    # callback_manager=callback_manager,\n","    n_ctx=4096,\n","    streaming = True,\n","    verbose=True,\n",")"]},{"cell_type":"markdown","metadata":{"id":"spoP_7ZYqfH3"},"source":["### open source instruct fine tuned model has its chat template. You might find using normal template of chat will aslo work in most cases, but that is not accurate."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3x4gq69jTNDv"},"outputs":[],"source":["class MistralChatbot:\n","    def __init__(self, llm):\n","        self.llm = llm\n","        self.messages = []\n","\n","    def __generate_prompt(self):\n","        if len(self.messages) != 0:\n","            prompt = \"<s>\"\n","            for i in self.messages:\n","                if i[\"role\"] == \"user\":\n","                    prompt += \"[INST]\"\n","                    prompt += i[\"content\"]\n","                    prompt += \"[/INST]\"\n","                elif i[\"role\"] == \"assistant\":\n","                    prompt += i[\"content\"]\n","                    prompt += \"</s>\"\n","            return prompt\n","\n","    def chat(self, msg):\n","        self.messages.append({\"role\": \"user\", \"content\": msg})\n","        prompt = self.__generate_prompt()\n","        res = llm.predict(prompt)\n","        self.messages.append({\"role\": \"assistant\", \"content\": res})\n","        return res"]},{"cell_type":"markdown","metadata":{"id":"YuX5L6jOq7aY"},"source":["### this session is optional, if you want to directly chat with the LLM with text rather than audio"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ONtzm01xJt79"},"outputs":[],"source":["# import gradio as gr\n","# import random\n","# import time\n","\n","# with gr.Blocks() as demo:\n","#     chatbot = gr.Chatbot()\n","#     msg = gr.Textbox()\n","#     clear = gr.ClearButton([msg, chatbot])\n","\n","#     def respond(message, chat_history):\n","#         bot_message = mc.chat(message)\n","#         chat_history.append((message, bot_message))\n","#         return \"\", chat_history\n","\n","#     msg.submit(respond, [msg, chatbot], [msg, chatbot])\n","\n","# demo.launch()"]},{"cell_type":"markdown","metadata":{"id":"0wMO_XiRrEDT"},"source":["### text to speech service, opne source silero is used here, code sample referred to https://github.com/ouoertheo/silero-api-server"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PmDQ4aCNkDqJ"},"outputs":[],"source":["import os, time\n","import shutil\n","import requests\n","import torch\n","import torch.package\n","import torchaudio\n","from hashlib import md5\n","from loguru import logger\n","from pydub import AudioSegment\n","from pathlib import Path\n","import json\n","\n","class SileroTtsService:\n","    \"\"\"\n","    Generate TTS wav files using Silero\n","    \"\"\"\n","    def __init__(self, sample_path, lang=\"v3_en.pt\") -> None:\n","        self.sample_text = \"The fallowed fallen swindle auspacious goats in portable power stations.\"\n","        self.sample_path = Path(sample_path)\n","        self.sessions_path = None\n","\n","        # Silero works fine on CPU\n","        self.device = torch.device('cpu')\n","        torch.set_num_threads(4)\n","        torchaudio.set_audio_backend(\"soundfile\")\n","\n","        # Make sure we have the sample path\n","        if not self.sample_path.exists():\n","            self.sample_path.mkdir()\n","\n","        self.sample_rate = 48000\n","        logger.info(f\"TTS Service loaded successfully\")\n","\n","        # Prevent generation failure due to too long input\n","        self.max_char_length = 600\n","\n","        # Get language model URLs\n","        self.langs = self.list_languages()\n","\n","        # Load model\n","        self.load_model(lang)\n","\n","    def init_sessions_path(self, sessions_path=\"sessions\"):\n","        self.sessions_path = Path(sessions_path)\n","        if not self.sessions_path.exists():\n","            self.sessions_path.mkdir()\n","\n","    def load_model(self, lang_model=\"v3_en.pt\"):\n","        # Download the model. Default to en.\n","        if lang_model not in self.langs:\n","            raise Exception(f\"{lang_model} not in {list(self.langs.values())}\")\n","\n","        model_url = self.langs[lang_model]\n","        self.model_file = Path(lang_model)\n","\n","        if not Path.is_file(self.model_file):\n","            logger.warning(f\"Downloading Silero {lang_model} model...\")\n","            torch.hub.download_url_to_file(model_url,\n","                                        self.model_file)\n","            logger.info(f\"Model download completed.\")\n","\n","        self.model = torch.package.PackageImporter(self.model_file).load_pickle(\"tts_models\", \"model\")\n","        self.model.to(self.device)\n","\n","    def generate(self, speaker, text, session=\"\"):\n","        if len(text) > self.max_char_length:\n","            # Handle long text input\n","            text_chunks = self.split_text(text)\n","            combined_wav = AudioSegment.empty()\n","\n","            for chunk in text_chunks:\n","                audio_path = Path(self.model.save_wav(text=chunk,speaker=speaker,sample_rate=self.sample_rate))\n","                combined_wav += AudioSegment.silent(500) # Insert 500ms pause\n","                combined_wav += AudioSegment.from_file(audio_path)\n","\n","            combined_wav.export(\"test.wav\", format=\"wav\")\n","            audio_path = Path(\"test.wav\")\n","        else:\n","            audio_path = Path(self.model.save_wav(text=text,speaker=speaker,sample_rate=self.sample_rate))\n","        if session:\n","            self.save_session_audio(audio_path, session, speaker)\n","        return audio_path\n","\n","    def split_text(self, text:str) -> list[str]:\n","        # Split text into chunks less than self.max_char_length\n","        chunk_list = []\n","        chunk_str = \"\"\n","\n","        for word in text.split(' '):\n","            word = word.replace('\\n',' ') + \" \"\n","            if len(chunk_str + word) > self.max_char_length:\n","                chunk_list.append(chunk_str)\n","                chunk_str = \"\"\n","            chunk_str += word\n","\n","        # Add the last chunk\n","        if len(chunk_str) > 0:\n","            chunk_list.append(chunk_str)\n","\n","        return chunk_list\n","\n","\n","    def combine_audio(self, audio_segments):\n","        combined_audio = AudioSegment.from_mono_audiosegments(audio_segments)\n","        return combined_audio\n","\n","    def save_session_audio(self, audio_path:Path, session:Path, speaker):\n","        if not self.sessions_path:\n","            raise Exception(\"Session not initialized. Call /tts/init_session with {'path':'desired\\session\\path'}\")\n","        session_path = self.sessions_path.joinpath(session)\n","        if not session_path.exists():\n","            session_path.mkdir()\n","        dst = session_path.joinpath(f\"tts_{session}_{int(time.time())}_{speaker}_.wav\")\n","        shutil.copy(audio_path, dst)\n","\n","    def get_speakers(self):\n","        \"List different speakers in model\"\n","        return self.model.speakers\n","\n","    def generate_samples(self):\n","        \"Remove current samples and generate new ones for all speakers.\"\n","        logger.warning(\"Removing current samples\")\n","        for file in self.sample_path.iterdir():\n","            os.remove(self.sample_path.joinpath(file))\n","\n","        logger.info(\"Creating new samples. This should take a minute...\")\n","        for speaker in self.model.speakers:\n","            sample_name = Path(self.sample_path.joinpath(f\"{speaker}.wav\"))\n","            if sample_name.exists():\n","                continue\n","            audio = Path(self.model.save_wav(text=self.sample_text,speaker=speaker,sample_rate=self.sample_rate))\n","            audio.rename(self.sample_path.joinpath(sample_name))\n","        logger.info(\"New samples created\")\n","\n","    def update_sample_text(self,text: str):\n","        \"Update the text used to generate samples\"\n","        if not text: return\n","        self.sample_text = text\n","        logger.info(f\"Sample text updated to {self.sample_text}\")\n","\n","    def list_languages(self):\n","        'Grab all v3 model links from https://models.silero.ai/models/tts'\n","        lang_file = Path('langs.json')\n","        if lang_file.exists():\n","            with lang_file.open('r') as fh:\n","                logger.info('Loading cached language index')\n","                return json.load(fh)\n","        logger.info('Loading remote language index')\n","        lang_base_url = 'https://models.silero.ai/models/tts'\n","        lang_urls = {}\n","\n","        # Parse initial web directory for languages\n","        response = requests.get(lang_base_url)\n","        langs = [lang.split('/')[0] for lang in response.text.split('<a href=\"')][1:]\n","\n","        # Enter each web directory and grab v3 model file links\n","        for lang in langs:\n","            response = requests.get(f\"{lang_base_url}/{lang}\")\n","            if not response.ok:\n","                raise f\"Failed to get languages: {response.status_code}\"\n","            lang_files = [f.split('\"')[0] for f in response.text.split('<a href=\"')][1:]\n","\n","            # If a valid v3 file, add to list\n","            for lang_file in lang_files:\n","                if lang_file.startswith('v3'):\n","                    lang_urls[lang_file]=f\"{lang_base_url}/{lang}/{lang_file}\"\n","        with open('langs.json','w') as fh:\n","            json.dump(lang_urls,fh)\n","        return lang_urls"]},{"cell_type":"markdown","metadata":{"id":"54-b0_hCrYEm"},"source":["### the final demo UI code is referred to https://github.com/ssheng/BentoChain\n","the process might will take sometime for text to audio generation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lGN0SIdm00Yk"},"outputs":[],"source":["from tempfile import tempdir\n","import gradio as gr\n","import whisper\n","import scipy.io.wavfile as wav\n","from threading import Lock\n","\n","tts_service = SileroTtsService(\"./samples\")\n","mc = MistralChatbot(llm)\n","\n","class ChatWrapper:\n","    def __init__(self, generate_speech, generate_text):\n","        self.lock = Lock()\n","        self.generate_speech = generate_speech\n","        self.generate_text = generate_text\n","\n","    def __call__(\n","        self,\n","        audio_path,\n","        text_message,\n","        history,\n","        chain\n","    ):\n","        self.lock.acquire()\n","\n","        # print(f\"audio_path : {audio_path} ({type(audio_path)})\")\n","        # print(f\"text_message : {text_message} ({type(text_message)})\")\n","\n","        try:\n","            model = whisper.load_model(\"base\")\n","            audio = whisper.load_audio(audio_path)\n","            temp = whisper.transcribe(model, audio=audio)\n","            transcription = temp[\"text\"]\n","            # print(f\"transcription : {transcription}\")\n","            history = history or []\n","\n","            output = mc.chat(transcription.strip())\n","            # print(f\"llm output : {output}\")\n","            history.append((transcription, output))\n","\n","            tts_service.generate(speaker = f'en_{49}', text=output)\n","            a, b = wav.read(\"test.wav\")\n","\n","        except Exception as e:\n","            print(e)\n","            raise e\n","        finally:\n","            self.lock.release()\n","        return history, history, (a,b), None, None\n","\n","def create_block(chat):\n","    \"\"\"Create the gradio block.\"\"\"\n","\n","    block = gr.Blocks(css=\".gradio-container\")\n","\n","    with block:\n","        with gr.Row():\n","            gr.Markdown(\"<h3><center>Chat with an AI Assistant via Audio</center></h3>\")\n","\n","        chatbot = gr.Chatbot()\n","\n","        audio = gr.Audio(label=\"Chatbot Voice\", elem_id=\"chatbox_voice\", autoplay=True)\n","\n","        with gr.Row():\n","            audio_message = gr.Audio(\n","                label=\"User voice message\",\n","                source=\"microphone\",\n","                type=\"filepath\",\n","            )\n","\n","            text_message = gr.Text(\n","                label=\"User text message\",\n","                placeholder=\"Type the message to llm\",\n","            )\n","\n","\n","        state = gr.State()\n","        agent_state = gr.State()\n","\n","\n","        text_message.submit(\n","            chat,\n","            inputs=[\n","                audio_message,\n","                text_message,\n","                state,\n","                agent_state,\n","            ],\n","            outputs=[chatbot, state, audio, audio_message, text_message],\n","            show_progress=False,\n","        )\n","\n","\n","        audio_message.change(\n","            chat,\n","            inputs=[\n","                audio_message,\n","                text_message,\n","                state,\n","                agent_state,\n","            ],\n","            outputs=[chatbot, state, audio, audio_message, text_message],\n","            show_progress=False,\n","        )\n","\n","        return block\n","\n","chat = ChatWrapper(None, None)\n","a=create_block(chat)\n","a.queue().launch(share=True, debug=True)"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
